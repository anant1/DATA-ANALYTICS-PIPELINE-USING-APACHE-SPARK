{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Background: The nytimes api doesn't returns full article body, just the snippet of it each returned\n",
    "# so, I have to write the code to fetch the weburl for each article, then perfrom further requests to those url and \n",
    "# fetch the story body of each article\n",
    "\n",
    "import requests # for performing html request to nytimes API\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import lxml.html as html # for scrapping the content of the article URL of nytimes\n",
    "import json\n",
    "\n",
    "# Each API keys can provide 10000 response hits\n",
    "topic = {\"Sports\":['sports','football','basketball','cricket','hockey','soccer','golf','tennis','worldcup'],\n",
    "         \"Business\":['business','deal','market','economy','money','entrepreneurship'],\n",
    "         \"Politics\":['politics','trump','modi','election','republican','democrats','white+house'],\n",
    "         \"Entertainment\":['entertainment','movie','dance','music','theater','shows']\n",
    "         }\n",
    "\n",
    "#topic = \"sports\"#topic to be looked for\n",
    "apikey = [\"2c94bb3581cf469fb33321a9e3bbac38\"]#,\"f4918bd3047241889c282af42bd2128a\",\"ddfe36f13f354e348d9cea28e2b27001\",\"6b23ec96494142948d419fd221150759\",\"527c18ffc4e648cb936f582a0e264ff1\"]\n",
    "fl = \"snippet,web_url\"#selective attributes of json response\n",
    "pageNo = \"0\"#initial page is 0, articles fetched using api are grouped in 10 per page starting 0 and upto page 100\n",
    "dateRange = [\"20180429\",\"20180430\",\"20180501\", \"20180502\", \"20180503\", \"20180504\", \"20180505\", \"20180506\", \"20180507\", \"20180508\"]# can be changed t any period\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function parse the json response from nytimes and create new dictionary using two attributes only\n",
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    news = []\n",
    "    fetch = articles['response']['docs']\n",
    "    for i in range(0,len(fetch)):\n",
    "        dic = {}\n",
    "#         print(fetch[i])\n",
    "        dic['web_url'] = fetch[i]['web_url']\n",
    "        if fetch[i]['snippet'] is not None:\n",
    "            dic['snippet'] = fetch[i]['snippet']\n",
    "#         dic['url'] = i['web_url']\n",
    "        news.append(dic)\n",
    "    return(news)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# caller function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function perfrom request to nytimes api using the paramters passed and returns the parseed responsed to the caller function\n",
    "def get_articles(topic, begin_date, end_date, fl, apikey):\n",
    "    all_articles = []#stores all articles for a particular day\n",
    "    page = 0\n",
    "#     i = 0\n",
    "    while(page<100):\n",
    "        sleep(1)\n",
    "#     for page in range(0,100): #NYT limits pages to first 100 pages starting page 0, each page has 10 articles max\n",
    "        try:\n",
    "            #apikey = apikeys[i]\n",
    "            url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\"+topic+\"&begin_date=\"+begin_date+\"&end_date=\"+end_date+\"&fl=\"+fl+\"&page=\"+str(page)+\"&api-key=\"+apikey\n",
    "            print(url)\n",
    "            requestArticles = requests.get(url)\n",
    "            data = requestArticles.json()\n",
    "            if len(data[\"response\"][\"docs\"])>0:\n",
    "                all_articles.append(parse_articles(data))\n",
    "#                 print(data)\n",
    "            else:# checks if further pages have no articles to show, if yes then break the loop and return the fetched articles\n",
    "                print(parse_articles(data))\n",
    "                break\n",
    "        except:\n",
    "            print(\"You called the api way to fast, Dude, trying again\")\n",
    "            #print(data)\n",
    "            sleep(1)\n",
    "#             page = page - 1\n",
    "            # if i<4:\n",
    "            #     i = i+1\n",
    "            continue#try again\n",
    "        print(\"Page: \"+str(page))\n",
    "#         break\n",
    "        page=page+1\n",
    "    return(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for Data period: 04-29-2018 - 04-30-2018onsportsof categorySports\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 1\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=2&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 2\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=3&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 3\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=4&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=4&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 4\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=5&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 5\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=6&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 6\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=7&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 7\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=8&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 8\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=9&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 9\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=10&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 10\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=11&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 11\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=12&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 12\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=sports&begin_date=20180429&end_date=20180430&fl=snippet,web_url&page=13&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FinalData/Sports/0sports20180429-full.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f869056972d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessArticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0marticles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FinalData/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdateRange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-full.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#used for storing full articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                             \u001b[0mfullpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"web_url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FinalData/Sports/0sports20180429-full.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "processArticles = []\n",
    "for k,v in topic.items():\n",
    "    for topic in v:\n",
    "        for i in range(0,9):\n",
    "            datetimeobject = datetime.strptime(dateRange[i],'%Y%m%d')\n",
    "            beginDate = datetimeobject.strftime('%m-%d-%Y')\n",
    "            datetimeobject = datetime.strptime(dateRange[i+1],'%Y%m%d')\n",
    "            endDate = datetimeobject.strftime('%m-%d-%Y')\n",
    "\n",
    "            print(\"Fetching articles for Data period: \" + beginDate + \" - \"+ endDate + \"on\" + topic + \"of category\" + k)\n",
    "            processArticles = get_articles(topic, dateRange[i], dateRange[i+1],fl)\n",
    "            if(len(processArticles)>0):\n",
    "        #         with open(\"FinalData/\"+k+'/'+topic+dateRange[i]+\".txt\", 'w') as outfile:#used for storing snippets\n",
    "        #             for item in processArticles:\n",
    "        #                 for articles in item:\n",
    "        # #                     print(articles)\n",
    "        # \t\t    dta = str(articles[\"snippet\"].encode(\"ascii\", \"ignore\"))\n",
    "        #                     outfile.write(dta)\n",
    "        #                     outfile.write(\"\\n\")\n",
    "                counter = 0\n",
    "                for item in processArticles:\n",
    "                    for articles in item:\n",
    "                        with open(\"FinalData/\"+k+'/'+str(counter)+topic+dateRange[i]+\"-full.txt\", 'w') as outfile:#used for storing full articles\n",
    "                            fullpage = requests.get(articles[\"web_url\"])\n",
    "                            print()\n",
    "                            htmlbody = html.fromstring(requests.get(articles[\"web_url\"]).content)\n",
    "                            output = \"\".join(htmlbody.xpath('//p[contains(@class,\"story-body-text\")]//text()'))#scrapper\n",
    "        #                     print(output+\"\\n\\n\")\n",
    "        #                     output = output.decode('utf8').encode('latin1').decode('utf8')\n",
    "                            output = str(output.encode(\"ascii\", \"ignore\"))#since the body has lots of escape characters, I have to convert\n",
    "            # utf-8 response into ascii using ignore flag to bypass those escape characters\n",
    "                            outfile.write(output[2:-1])\n",
    "                            outfile.write(\"\\n\")\n",
    "                            counter = counter + 1\n",
    "            else:\n",
    "                print(\"Insufficient data for date: \"+beginDate+\" to save\")\n",
    "\n",
    "        # /usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.8.3.jar -input /user/jayant/input/ -output /user/jayant/output_new2 -mapper /home/jayant/wordcount_mapper.py -reducer /home/jayant/wordcount_reducer.py -numReduceTasks 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
